# Interstice V1.0 Implementation Plan

## Overview

This document outlines detailed, prioritized steps to achieve a stable, production-ready V1.0 release of Interstice. The plan is organized into **4 phases** with clear milestones, dependencies, and success criteria.

**Target:** A minimal but complete system that reliably runs modules with persistence, tooling, and predictable behavior.

---

## Phase 1: Persistence & Durability (Critical Foundation)
**Est. Duration:** 3-4 weeks  
**Dependency:** Core Runtime (✓ already done)  
**Milestone:** Transactions survive process restart

### 1.1 Append-Only Transaction Log
**Status:** Not started  
**Tasks:**
- [x] Design transaction log format (row operations + metadata)
  - Record type (Insert, Update, Delete)
  - Module ID + table ID + row data
  - Timestamp (logical clock or Unix timestamp)
  - Transactional boundaries
- [ ] Implement `TransactionLog` struct in `interstice-core`
  - Append operation with atomic file writes
  - Sequential read interface
  - Checksum/CRC validation per record
- [ ] Implement log rotation strategy
  - Max log size before rotation
  - Rotate to `txn.log.0`, `txn.log.1`, etc.
  - Configurable retention policy

**Success Criteria:**
- All reducer transactions are written to disk before acknowledging commit
- Log format is stable (versioned)
- Disk writes are atomic (use atomic rename or similar)

---

### 1.2 Persistence of Committed Transactions
**Status:** Not started  
**Tasks:**
- [ ] Integrate `TransactionLog` into the main runtime
  - On each reducer call that mutates tables, log the mutation
  - Only after write succeeds, ack the mutation to the module
- [ ] Add configuration for log persistence
  - Enable/disable per runtime
  - Path to log directory
  - Durability guarantees (sync frequency)
- [ ] Handle concurrent writes safely
  - Use a `Mutex<File>` or similar for log appends
  - Consider buffering strategy (flush frequency)

**Success Criteria:**
- Mutations are durable immediately after commit
- No in-memory-only mutations escape the reducer
- Config allows tuning durability vs. performance

---

### 1.3 Replay Engine
**Status:** Not started  
**Tasks:**
- [ ] Build `ReplayEngine` struct
  - Reads transaction log sequentially
  - Reconstructs table state by replaying mutations
  - Skips reducer execution (pure mutations only)
  - Disables subscriptions during replay
- [ ] Implement state reconstruction
  - Create empty tables from module schemas
  - Apply all logged mutations in order
  - Validate final state matches expected snapshots (if present)
- [ ] Integrate into startup path
  - On runtime initialization, check for existing log
  - If log exists, run replay to restore state
  - Otherwise, initialize fresh
- [ ] Handle replay errors gracefully
  - Log replay failures with context (which transaction, why)
  - Optionally skip corrupted transactions (with warning)
  - Provide recovery mode (e.g., `--skip-to-log-offset`)

**Success Criteria:**
- State after replay matches state before shutdown
- Reducer cycles during replay are detected and prevented
- Replay is significantly faster than normal execution

---

### 1.4 Disable Subscriptions During Replay
**Status:** Not started  
**Tasks:**
- [ ] Add `ReplayContext` flag to scheduler
  - During replay, subscriptions are queued but not executed
  - After replay completes, flush subscription queue
- [ ] Implement subscription deferral
  - Buffer subscription events during replay
  - Execute buffered subscriptions in deterministic order post-replay

**Success Criteria:**
- No subscription reducers run during replay
- Subscriptions fire correctly after replay completes

---

### 1.5 Skip Reducer Execution During Replay
**Status:** Not started (partially implied by 1.3)  
**Tasks:**
- [ ] Mark replay phase in runtime state
  - `is_replaying: bool` flag
  - Only apply mutations, do not invoke reducer logic
- [ ] Validate that no WASM is executed during replay

**Success Criteria:**
- WASM is never instantiated or called during replay
- Performance is O(log size), not O(reducer complexity)

---

### 1.6 Log Integrity Verification
**Status:** Not started  
**Tasks:**
- [ ] Add checksums to transaction log records
  - CRC32 per record or SHA256 per batch
  - Verify on read
- [ ] Implement log validation CLI
  - `interstice check-log <log_path>`
  - Report any corrupted records
  - Optionally repair (truncate to last valid record)
- [ ] Add recovery mode documentation

**Success Criteria:**
- Corrupted records are detected
- System can recover from partial writes

---

### 1.7 Schema Compatibility Checks During Replay
**Status:** Not started  
**Tasks:**
- [ ] Store schema versions in log
  - Snapshot schema at time of mutation
  - Store version hash with each record
- [ ] On replay, validate schema compatibility
  - If schema changed, ensure backward compatibility
  - Warn or error if incompatible (configurable)
- [ ] Document schema migration strategy
  - How to safely update schemas
  - Manual migration reducer pattern

**Success Criteria:**
- Replay aborts with clear error if schema incompatibility detected
- Documented path for safe schema updates

---

## Phase 2: Tables & Storage Enhancements
**Est. Duration:** 2-3 weeks  
**Dependency:** Phase 1  
**Milestone:** Efficient queries on large datasets

### 2.1 Indexed Tables
**Status:** Not started  
**Tasks:**
- [ ] Design index interface
  - Primary key index (existing, but formalize)
  - Secondary indexes (by column)
  - Composite indexes
- [ ] Implement `Index` trait
  - `fn index_on(&self, column: &str)`
  - `fn query_indexed(&self, index: &str, value: &Value) -> Vec<RowId>`
- [ ] Add index metadata to schema
  - Define indexes in `#[table]` macro
  - Example: `#[index(name = "user_by_email")]`

**Success Criteria:**
- Queries on indexed columns are O(log n) not O(n)
- Indexes are automatically updated on mutations

---

### 2.2 Efficient Table Scans
**Status:** Not started  
**Tasks:**
- [ ] Implement iterator-based table scans
  - `fn scan(&self) -> impl Iterator<Item = &Row>`
  - Avoid cloning entire table
- [ ] Add filtering during iteration
  - Predicate-pushdown (filters applied before returning rows)
  - Example: `scan().filter(|row| row.age > 18)`

**Success Criteria:**
- Large table scans do not allocate O(n) memory
- Filtering is efficient

---

### 2.3 Columnar / Structured Storage Backend
**Status:** Not started  
**Tasks:**
- [ ] Design columnar layout option
  - Store by column instead of by row
  - Better cache locality and compression
- [ ] Implement optional columnar backend
  - Add to `StorageBackend` trait
  - Default remains row-oriented
  - Enable via configuration
- [ ] Benchmark row vs. columnar
  - Profile with realistic workloads

**Success Criteria:**
- Columnar backend is optional and transparent to modules
- Benchmarks show improvements for certain workloads (e.g., aggregations)

---

### 2.4 Table Migration Support
**Status:** Not started  
**Tasks:**
- [ ] Design migration system
  - Migrations are reducers that transform table state
  - Named and versioned migrations
  - Applied in order during replay/initialization
- [ ] Implement migration registration
  - `#[migration(from_version = 1, to_version = 2)]`
  - Reducer function signature for migrations
- [ ] Add migration tracking to log
  - Record which migrations have been applied
  - Prevent re-running migrations

**Success Criteria:**
- Schemas can be evolved safely
- Old data is automatically transformed

---

### 2.5 Table Schema Versioning
**Status:** Partially done (modules have versions, tables don't explicitly)  
**Tasks:**
- [ ] Extend table schema with version field
  - Each table definition gets a version number
  - Migrations increment version
- [ ] Update module interface to include per-table versions
  - Exported as part of module schema
  - Callers must respect table versions

**Success Criteria:**
- Table versions are explicit in the interface
- Version mismatches are detected at load time

---

## Phase 3: SDK Ergonomics & Type System
**Est. Duration:** 2-3 weeks  
**Dependency:** Phase 2  
**Milestone:** Module authors rarely see raw `IntersticeValue`

### 3.1 Typed Table Handles
**Status:** Not started  
**Tasks:**
- [ ] Design `TableHandle<T>` generic type
  - Wraps table operations with compile-time type info
  - `insert(&self, row: T) -> Result<RowId, Error>`
  - `query(&self, filter: impl Fn(&T) -> bool) -> Vec<T>`
- [ ] Extend `#[table]` macro to generate typed handle
  - Generates `pub type MyTableHandle = TableHandle<MyRow>`
- [ ] Update SDK documentation with examples

**Success Criteria:**
- Module code avoids `IntersticeValue` casts
- IDE autocomplete works for table operations

---

### 3.2 Typed Reducer Calls
**Status:** Not started  
**Tasks:**
- [ ] Design typed reducer call interface
  - `call_reducer::<InputType, OutputType>(...) -> Result<OutputType, Error>`
  - Compile-time signature validation if possible
- [ ] Extend `#[reducer]` macro
  - Generate typed call helper
  - Validate argument and return types at schema generation time
- [ ] Emit TypeScript/JSON schema for cross-language support

**Success Criteria:**
- Reducer calls have explicit input/output types
- Type mismatches are caught early

---

### 3.3 Compile-Time Reducer & Table Name Validation
**Status:** Not started  
**Tasks:**
- [ ] Store reducer/table definitions in a registry during macro expansion
  - Build time check or runtime schema validation
- [ ] Generate compile-time const for reducer/table names
  - Prevent typos: use `MyModule::REDUCER_FOO_NAME` instead of string
- [ ] Provide linter/plugin for IDE to catch invalid references

**Success Criteria:**
- Typos in reducer/table names are caught at compile time or early runtime
- Valid name suggestions are shown in IDE

---

### 3.4 Reducer Context Abstraction
**Status:** Partially done (context exists, can be improved)  
**Tasks:**
- [ ] Review current `Context` API
  - Identify overly low-level operations
  - Design ergonomic alternatives
- [ ] Add context builder for cleaner initialization
  - `ContextBuilder::new().with_table_handle(...).build()`
- [ ] Add helper methods
  - `context.now()` -> timestamp
  - `context.module_id()` already exists
  - `context.call_reducer::<T, U>()` with typed support

**Success Criteria:**
- Reducers rarely construct context directly
- Idiomatic Rust patterns are used

---

### 3.5 Read-Only vs Read-Write Contexts
**Status:** Not started  
**Tasks:**
- [ ] Create `ReadContext` and `WriteContext` types
  - `ReadContext` allows only queries
  - `WriteContext` allows mutations
- [ ] Update `#[reducer]` macro to declare mode
  - `#[reducer(mode = "read")]` for pure queries
  - `#[reducer(mode = "write")]` for mutations (default)
- [ ] Enforce at compile time (via type system)

**Success Criteria:**
- Read-only reducers cannot accidentally mutate
- Compiler prevents misuse

---

### 3.6 Eliminate Direct `IntersticeValue` Usage
**Status:** Not started  
**Tasks:**
- [ ] Audit SDK for places where users must construct `IntersticeValue`
  - Dynamic type construction
  - Custom serialization
- [ ] Provide higher-level abstractions for common cases
  - `Value::from_struct()` helper
  - Serde integration for derived types
- [ ] Document when manual `IntersticeValue` is necessary

**Success Criteria:**
- 95% of module code avoids `IntersticeValue` directly
- Documentation provides clear patterns

---

### 3.7 Custom Types & Type System
**Status:** Not started  
**Tasks:**
- [ ] Implement `#[derive(IntersticeType)]` macro
  - Generates schema info for custom types
  - Validates supported field types at compile time
- [ ] Support common patterns
  - Structs with named fields
  - Enums (sum types)
  - `Option<T>` and `Vec<T>`
- [ ] Compile-time rejection of unsupported layouts
  - Custom types must be representable in the ABI
  - Error messages guide users to supported patterns

**Success Criteria:**
- Module authors can define domain-specific types
- Unsupported types fail at compile time with clear messages

---

### 3.8 Automatic Schema Generation for Custom Types
**Status:** Not started  
**Tasks:**
- [ ] Extend `#[derive(IntersticeType)]` to emit schema
  - Schema JSON or bytecode representation
  - Versioned alongside module
- [ ] Register custom type schemas in module interface
  - Imported by calling modules
  - Type-safe cross-module interaction

**Success Criteria:**
- Custom types are part of exported module schema
- Other modules can use custom types from dependencies

---

## Phase 4: Tooling, Observability & Stability
**Est. Duration:** 2-3 weeks  
**Dependency:** Phases 1-3  
**Milestone:** Production-ready debugging and deployment

### 4.1 Schema Inspection CLI
**Status:** Not started  
**Tasks:**
- [ ] Create CLI command `interstice schema <module_path>`
  - Lists all tables, reducers, subscriptions
  - Shows versioning info
  - Pretty-print schema
- [ ] Add output formats: JSON, YAML, human-readable
- [ ] Implement diff command: `interstice schema-diff <old_path> <new_path>`
  - Highlight compatible vs. breaking changes

**Success Criteria:**
- Module interface is easily inspectable without running the module
- Version compatibility is clearly shown

---

### 4.2 Module Validation CLI
**Status:** Not started  
**Tasks:**
- [ ] Create `interstice validate <module_path>`
  - Checks WASM validity
  - Verifies module matches expected ABI version
  - Reports any schema errors
- [ ] Dry-run module load without execution
  - Verify dependencies are satisfied
  - Check for missing capabilities

**Success Criteria:**
- Modules can be validated before deployment
- Clear error messages for common mistakes

---

### 4.3 Transaction Log Inspection
**Status:** Not started  
**Tasks:**
- [ ] Create `interstice log-inspect <log_path>`
  - Display all transactions in human-readable format
  - Filter by module/table/time range
  - Show mutation sequences
- [ ] Add `log-dump` for raw export
  - CSV, JSON, or binary dump
  - Useful for analysis and debugging

**Success Criteria:**
- Transaction history can be examined offline
- Debugging complex state issues is feasible

---

### 4.4 Replay & Determinism Checker
**Status:** Not started  
**Tasks:**
- [ ] Create `interstice replay-check <log_path> <module_set>`
  - Replays entire log
  - Verifies determinism (run twice, compare results)
  - Reports any non-deterministic behavior
- [ ] Add `replay-debug` with tracing
  - Step through replay with breakpoints
  - Inspect state at each transaction

**Success Criteria:**
- Determinism violations are caught and reported
- Debugging non-deterministic reducers is possible

---

### 4.5 Dev-Mode Tracing & Logging
**Status:** Partially done (logging exists, enhance it)  
**Tasks:**
- [ ] Implement structured logging
  - Use `tracing` crate or similar
  - Log levels: trace, debug, info, warn, error
  - Context propagation (module ID, reducer name, etc.)
- [ ] Add dev-mode output option
  - `--log-level debug` for verbose output
  - `--log-format json` for structured parsing
- [ ] Reducer execution tracing
  - Log reducer entry/exit
  - Log table mutations with before/after snapshots
  - Optional flame graph generation

**Success Criteria:**
- Complex execution bugs can be debugged with logs
- Performance profiling is straightforward

---

### 4.6 Hot-Reload Modules (Optional)
**Status:** Not started  
**Priority:** Low (defer to post-V1.0 if time)  
**Tasks:**
- [ ] Design hot-reload semantics
  - What state is preserved across reload?
  - How are subscriptions updated?
  - Graceful handling of schema changes
- [ ] Implement module reload API
  - `interstice.reload_module(module_id, new_wasm)`
  - Validate compatibility
  - Preserve tables, drop subscriptions, reinit
- [ ] Add CLI command
  - `interstice hot-reload <module_id> <new_wasm_path>`

**Success Criteria:**
- Modules can be updated without full restart
- State is preserved across reloads where safe

---

### 4.7 Structured Logging
**Status:** Partially done  
**Tasks:**
- [ ] Adopt `tracing` + `tracing-subscriber`
  - Consistent logging infrastructure
  - Configurable output (console, file, JSON)
- [ ] Add module context to all logs
  - Module ID, reducer name, table name
  - Request tracing (follow a reducer call chain)
- [ ] Log important events
  - Module load/unload
  - Reducer execution (with args, return value)
  - Table mutations (before/after)
  - Subscription triggers

**Success Criteria:**
- All logs include consistent context
- Filtering logs by module/reducer/time is easy

---

### 4.8 Reducer Execution Tracing
**Status:** Not started  
**Tasks:**
- [ ] Instrument reducer dispatch
  - Record entry point (module, reducer name, args)
  - Record exit point (result or error)
  - Record elapsed time
- [ ] Generate execution trace
  - Call stack of reducer invocations
  - Dependency graph of modules involved
  - Timeline view of execution
- [ ] Export traces
  - Chrome DevTools format for visualization
  - JSON for offline analysis
  - Flame graph generation

**Success Criteria:**
- Complex reducer call chains can be visualized
- Performance hotspots are identifiable

---

### 4.9 Transaction Visualization
**Status:** Not started  
**Tasks:**
- [ ] Generate transaction graphs
  - Nodes: reducers, table mutations
  - Edges: dependencies (reducer A calls reducer B)
  - Timing information
- [ ] Create CLI command
  - `interstice graph-trace <log_path> <output.html>`
  - Interactive HTML visualization
- [ ] Export formats
  - GraphML for graph analysis tools
  - SVG for static documentation

**Success Criteria:**
- Complex transaction histories can be understood visually
- Debugging transaction order issues is easier

---

### 4.10 Subscription Trace Graphs
**Status:** Not started  
**Tasks:**
- [ ] Record subscription triggers
  - Which table mutation triggered which subscription?
  - Order of subscription execution
  - Subscription dependency chains
- [ ] Generate subscription dependency graph
  - Nodes: tables, subscriptions, reducers
  - Edges: subscription relationships
  - Cycles (if any)
- [ ] Detect subscription cycles
  - Error if circular subscription detected
  - Report cycle path

**Success Criteria:**
- Subscription logic can be debugged visually
- Circular dependencies are caught

---

### 4.11 Deterministic Replay Debugging
**Status:** Not started  
**Tasks:**
- [ ] Build interactive replay debugger
  - Step through replay one transaction at a time
  - Inspect table state at each step
  - Set breakpoints by transaction or condition
- [ ] Implement state snapshots
  - Save state at each transaction
  - Compare snapshots to detect divergence
- [ ] Export debugging session
  - Save snapshots and trace for later analysis

**Success Criteria:**
- Non-deterministic bugs can be pinpointed to specific transactions
- Reproducible step-by-step debugging is possible

---

## Phase 5: Testing & Validation
**Est. Duration:** 1-2 weeks (concurrent with other phases)  
**Milestone:** High confidence in core correctness

### 5.1 Unit Tests for Each Phase
**Status:** Ongoing  
**Tasks:**
- [ ] Phase 1: Transaction log integrity, replay correctness
- [ ] Phase 2: Index correctness, migration application
- [ ] Phase 3: Type validation, macro expansion
- [ ] Phase 4: CLI commands, visualization generation

---

### 5.2 Integration Tests
**Status:** In progress (expand)  
**Tasks:**
- [ ] Multi-module interaction scenarios
- [ ] Subscription chains
- [ ] Error handling and recovery
- [ ] Persistence and replay scenarios

---

### 5.3 Example Modules
**Status:** Partially done (hello, graphics, caller exist)  
**Tasks:**
- [ ] Expand examples to cover:
  - Multi-module reducer calls
  - Subscriptions with state propagation
  - Schema migrations
  - Custom types
  - Large datasets with indexing
- [ ] Create real-world-like example
  - E.g., a simple game or simulation
  - Demonstrates core patterns
  - Serves as documentation

---

### 5.4 Stress Testing
**Status:** Not started  
**Tasks:**
- [ ] Large transaction logs (millions of rows)
- [ ] High reducer invocation rate
- [ ] Complex subscription chains
- [ ] Module load/unload cycles
- [ ] Concurrent table access patterns

---

## Timeline & Dependencies

```
Phase 1 (Weeks 1-4): Persistence & Durability
  ├─ 1.1-1.3: Core log and replay
  ├─ 1.4-1.5: Subscription/execution handling
  └─ 1.6-1.7: Validation & schema compatibility

Phase 2 (Weeks 3-5): Tables & Storage
  ├─ 2.1-2.2: Indexes & efficient scans
  ├─ 2.3: Columnar backend (optional)
  └─ 2.4-2.5: Migrations & versioning

Phase 3 (Weeks 5-7): SDK Ergonomics
  ├─ 3.1-3.3: Typed APIs
  ├─ 3.4-3.6: Context & value abstractions
  └─ 3.7-3.8: Custom type system

Phase 4 (Weeks 7-9): Tooling & Observability
  ├─ 4.1-4.4: CLI commands
  ├─ 4.5-4.8: Logging & tracing
  └─ 4.9-4.11: Visualization & debugging

Phase 5 (Throughout): Testing & Validation
  └─ Continuous integration with other phases

Total Estimate: 8-10 weeks for core features
(Plus optional features and polish: 2-3 weeks)
```

---

## Success Criteria for V1.0

### Functional
- [x] WASM module loading and execution
- [ ] Durable persistence (Phase 1 complete)
- [ ] Correct replay from logs (Phase 1 complete)
- [ ] Typed, ergonomic SDK (Phase 3 complete)
- [ ] Complete table operations (Phase 2 complete)
- [ ] Working CLI tooling (Phase 4 complete)

### Non-Functional
- [ ] No breaking changes in v0.9.x -> v1.0.0 (acceptable, documented)
- [ ] Deterministic execution verified by tools
- [ ] Handles 1M+ row tables efficiently
- [ ] Supports complex multi-module applications
- [ ] Comprehensive documentation and examples

### Quality
- [ ] 80%+ code coverage for core
- [ ] All phases have integration tests
- [ ] Example modules demonstrate each major feature
- [ ] No known data loss or corruption bugs

---

## Known Unknowns & Risks

### Technical
- **Determinism at scale:** Does determinism hold under heavy subscription load?
  - Mitigation: Phase 4 determinism checker
- **Schema evolution:** How complex can migrations get?
  - Mitigation: Start simple, document patterns

### Design
- **Module interface stability:** Will v1.0 APIs feel clunky in 5 years?
  - Mitigation: Designed for versioning, can evolve
- **Performance:** Are the perf targets realistic?
  - Mitigation: Phase 5 stress testing

---

## Post-V1.0 (Not in this plan)

- Distributed execution
- Network transparency
- Authority system (beyond basic capabilities)
- Graphics module reference implementation
- Package manager / module registry
- Debugger UI (web-based)
- Performance optimizations (parallelism, zero-copy)

---

## How to Use This Plan

1. **Start with Phase 1** — Persistence is the foundation; nothing works without it.
2. **Parallel work:** While Phase 1 stabilizes, start Phase 2 design.
3. **Regular checkpoints:** After each sub-phase, run integration tests.
4. **Scope creep:** If a task doesn't directly enable V1.0 goals, defer it.
5. **Documentation:** Update README and Architecture docs as you go.

Each phase unlocks the next; completing all 4 yields a cohesive, stable system ready for early users.
